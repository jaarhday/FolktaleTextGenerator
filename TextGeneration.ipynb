{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaarhday/FolktaleTextGenerator/blob/main/TextGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## I. Parse Text Sources\n",
        "First we'll load our text sources and create our vocabulary lists and encoders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8RbnIjwHGoR",
        "outputId": "2c41b1b2-db65-4154-ba5f-b6dc570ebd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1917369 characters\n"
          ]
        }
      ],
      "source": [
        "# Load file data\n",
        "path_to_file = tf.keras.utils.get_file('tales.txt', 'https://raw.githubusercontent.com/jaarhday/FolktaleTextGenerator/main/complete_data.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XRnt0XUHUrq",
        "outputId": "61a381b0-63da-4280-fd93-66c3294cd66d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preface\n",
            "\n",
            "In the beginning of the New-making, the ancient fathers lived\n",
            "successively in four caves in the Four fold-containing-earth. The first\n",
            "was of sooty blackness, black as a chimney at night time;\n"
          ]
        }
      ],
      "source": [
        "# Verify the first part of our data\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SLd7l0HP1Po",
        "outputId": "08d55a6e-8a4a-462a-f007-81a03428a792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82 unique characters\n",
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ã', 'ä', 'é']\n"
          ]
        }
      ],
      "source": [
        "# Now we'll get a list of the unique characters in the file. This will form the\n",
        "# vocabulary of our network. There may be some characters we want to remove from this\n",
        "# set as we refine the network.\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NtjOxL7wQibb"
      },
      "outputs": [],
      "source": [
        "# Next, we'll encode encode these characters into numbers so we can use them\n",
        "# with our neural network, then we'll create some mappings between the characters\n",
        "# and their numeric representations\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True)\n",
        "\n",
        "# Here's a little helper function that we can use to turn a sequence of ids\n",
        "# back into a string:\n",
        "# turn them into a string:\n",
        "def text_from_ids(ids):\n",
        "  joinedTensor = tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n",
        "  return joinedTensor.numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52bkemreRw8q",
        "outputId": "a0a075a6-dd04-4d70-b6ff-403fbceaec8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([44, 71, 74, 73, 61])>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Now we'll verify that they work, by getting the code for \"A\", and then looking\n",
        "# that up in reverse\n",
        "testids = ids_from_chars([\"T\", \"r\", \"u\", \"t\", \"h\"])\n",
        "testids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGUnSHjtD_IJ",
        "outputId": "6d5c581a-f612-4d47-917b-1c561e3b27c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=string, numpy=array([b'T', b'r', b'u', b't', b'h'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "chars_from_ids(testids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8rghkpLLLjL5",
        "outputId": "fe9800fa-1a8a-4ca1-ce25-ed3fbbd429e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Truth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "testString = text_from_ids( testids )\n",
        "testString"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXMVqTcSpA0"
      },
      "source": [
        "## II. Construct our training data\n",
        "Next we need to construct our training data by building sentence chunks. Each chunk will consist of a sequence of characters and a corresponding \"next sequence\" of the same length showing what would happen if we move forward in the text. This \"next sequence\" becomes our target variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PLJWOg2P_fE",
        "outputId": "bd81aace-b773-466b-8380-3227a4b795da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1917369,), dtype=int64, numpy=array([40, 71, 58, ..., 75, 58,  3])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# First, create a stream of encoded integers from our text\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-nBqVY6pFpZs"
      },
      "outputs": [],
      "source": [
        "# Now, convert that into a tensorflow dataset\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8Fr28CJxUBtG"
      },
      "outputs": [],
      "source": [
        "# Finally, let's batch these sequences up into chunks for our training\n",
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Call the function for every sequence in our list to create a new dataset\n",
        "# of input->target pairs\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poNVukmsUFkq",
        "outputId": "e537894f-4730-4dc4-923d-c3563da129f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  Preface\n",
            "\n",
            "In the beginning of the New-making, the ancient fathers lived\n",
            "successively in four caves in\n",
            "--------\n",
            "Target:  reface\n",
            "\n",
            "In the beginning of the New-making, the ancient fathers lived\n",
            "successively in four caves in \n"
          ]
        }
      ],
      "source": [
        "# Verify our sequences\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input: \", text_from_ids(input_example))\n",
        "    print(\"--------\")\n",
        "    print(\"Target: \", text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDdr6xfZYa0o",
        "outputId": "1b14c999-3a37-499a-f718-6d23922dc577"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Finally, we'll randomize the sequences so that we don't just memorize the books\n",
        "# in the order they were written, then build a new streaming dataset from that.\n",
        "# Using a streaming dataset allows us to pass the data to our network bit by bit,\n",
        "# rather than keeping it all in memory. We'll set it to figure out how much data\n",
        "# to prefetch in the background.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "outputs": [],
      "source": [
        "class TextModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "model = TextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "3a3c0dd1-0a49-44a5-c68a-c7903747e3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "a3d4025b-6223-4088-9b24-2196946c8ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"text_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  21248     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  85075     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4044627 (15.43 MB)\n",
            "Trainable params: 4044627 (15.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "30170351-7a8e-429c-b79e-97565edbe47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "296/296 [==============================] - 23s 57ms/step - loss: 2.3712\n",
            "Epoch 2/25\n",
            "296/296 [==============================] - 18s 55ms/step - loss: 1.7302\n",
            "Epoch 3/25\n",
            "296/296 [==============================] - 19s 56ms/step - loss: 1.5108\n",
            "Epoch 4/25\n",
            "296/296 [==============================] - 19s 58ms/step - loss: 1.3984\n",
            "Epoch 5/25\n",
            "296/296 [==============================] - 19s 56ms/step - loss: 1.3293\n",
            "Epoch 6/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 1.2781\n",
            "Epoch 7/25\n",
            "296/296 [==============================] - 18s 55ms/step - loss: 1.2341\n",
            "Epoch 8/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 1.1939\n",
            "Epoch 9/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 1.1553\n",
            "Epoch 10/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 1.1170\n",
            "Epoch 11/25\n",
            "296/296 [==============================] - 19s 57ms/step - loss: 1.0784\n",
            "Epoch 12/25\n",
            "296/296 [==============================] - 21s 59ms/step - loss: 1.0389\n",
            "Epoch 13/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 1.0010\n",
            "Epoch 14/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.9621\n",
            "Epoch 15/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.9261\n",
            "Epoch 16/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.8930\n",
            "Epoch 17/25\n",
            "296/296 [==============================] - 19s 58ms/step - loss: 0.8618\n",
            "Epoch 18/25\n",
            "296/296 [==============================] - 18s 57ms/step - loss: 0.8352\n",
            "Epoch 19/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.8116\n",
            "Epoch 20/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.7916\n",
            "Epoch 21/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.7765\n",
            "Epoch 22/25\n",
            "296/296 [==============================] - 18s 57ms/step - loss: 0.7626\n",
            "Epoch 23/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.7511\n",
            "Epoch 24/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.7430\n",
            "Epoch 25/25\n",
            "296/296 [==============================] - 18s 56ms/step - loss: 0.7355\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "history = model.fit(dataset, epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "casEwxrXcv4Y"
      },
      "source": [
        "## V. Use the model\n",
        "\n",
        "Now that our model has been trained, we can use it to generate text. As mentioned earlier, to do so we have to keep track of its internal state, or memory, so that we can use previous text predictions to inform later ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V3lhlyfwcqIN"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return chars_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUghgUFc6ba",
        "outputId": "b888072c-86bc-4556-db2e-c76e865c9b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London.'\n",
            "\n",
            "\"'Alford I have nuts the earth on I'L Duv Lana'n, a snake's horse, I have searce!\"\n",
            "\n",
            "Wenonah, and such a point should be characters, and when ruin about\n",
            "the budges of brave joined claws, a soldier, velvious and person was\n",
            "then absent.\n",
            "\n",
            "That morning Gwynner, Apara snakes here in his ears and ducked it, and when she\n",
            "looked from its hunting cloak toward Puck to fight, and when he looked again\n",
            "looking up the mast extended on the stones and casts him down, for the\n",
            "populace of the people came to him in a vain.\n",
            "He gazed on his hands to feed through the shade, and rolled his\n",
            "hands beneath this dress, ridden flushed, steep cannoes from the\n",
            "water, and up in the dark, though he was going on a bridle until the\n",
            "pebble we had ever happened that he was affianced to them. But bluffy she pauses\n",
            "never that Little Daughter was the only light, he followed the streams at\n",
            "a city and near the others.\n",
            "\n",
            "\"Fionn saw you that I must apples in the kings:     I am any aished-blesspritte\n",
            "pond. It is very cool yet ready to change ours for a great distrust of other\n",
            "hamse by means of consciousness. In the forget-be when the strangers and\n",
            "curiosity was far beyond the telegraps, who call it as a candle sinking,\n",
            "left to each other, lonely underrands. The maiden was still. He flashed\n",
            "also, for he made him coming like them. With his wife, and even climbed out\n",
            "on to his magic. Some crowder was filled with swear he saw his enemy about\n",
            "him.\n",
            "\n",
            "Tusee like logs rose to the hill light charms.\n",
            "There were many little boat and chatted into the Wales, where the bitin\n",
            "of the lips were astonished as Corne America.\n",
            "\n",
            "It was how to live they slept. They put every moment of fresh windows, declared\n",
            "that as Aronn had always been forward Rate.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "NEVUTALE THE DUNDER FORES\n",
            "\n",
            "Ut they went into the house. A babber went back through the withered good\n",
            "village by husing-twouts to his, for lack, or a baby, one floor in 1778, when\n",
            "the will-o'-the-girl found five times rolled over-shadow. Marrison, that some\n",
            "of the lights were very madd. Then, in a twinkling it was proud for\n",
            "Conn Above Nicticak evening, as Corraga, baking three of his maid, for for some\n",
            "time with his rump separated five dollars and could not be safes of my robber\n",
            "might suich, knowing and fine monstery. The red strings he stopping to him and\n",
            "will put a council.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE GREAWS ARDORSPOS\n",
            "\n",
            "Long after, Cal.)\n",
            "\n",
            "Please, of honor of three men, a sharp coast, Harald replied, \"to learn again. Short I'd like to\n",
            "fill six rews or bright clouds along the very next?\" he continued, \"I had a\n",
            "badger in her fat. I thank you,\" they called. \"Bewstable in\n",
            "a mirror, and I will be your wedded. Mother Hanard saw it to\n",
            "Cleud under the helm. As it were returning through the veremaid: \"_ he saw\n",
            "you join you with me to yisles lively. \"Remember Harald sense change for\n",
            "hours came to my right, always a rising star flew out on memors. I have burn the shadow of old time,\" said Cuillen, and striking\n",
            "through the advantage of slender Island; and so earnessly they call it along\n",
            "the long strps.\"\n",
            "\n",
            "Some of the height was shown by the shawl of sick pals or voice. Bloody men\n",
            "called him his chosen dying moments, growing stars, to rush out\n",
            "and paddled and the slaves. When turning out these hills and strange conditions came\n",
            "in! Origial cried Art in life and heavy that they moved\n",
            "upon a field.\n",
            "\n",
            "My mother used to play away for a few scatsoes and cords and threw him\n",
            "gentle remains of pressure. Then he began to be striding over the\n",
            "turnips. They remained unrellowers west and rumpled and smart, and filling the red men of\n",
            "the people, he inhabited by the high seat. Nightly I found eventure know.\"\n",
            "\n",
            "\"If you ame all any minute,\" said Conn.\n",
            "\"ounly join the name,\" said Fionn.\n",
            "\n",
            "\"Years later,\" make this song of this island,\" Thorfinn said. \"It is all played.\"\n",
            "\n",
            "With this soul was ran inland out,--snatching as gush or flining of his men folk\n",
            "during the water, would move wanted her in secret, so that old\n",
            "Nack took his messing low, she smoted toward the ship. Collect makes fixe\n",
            "dug in shoes; until they sink into all sides he\n",
            "said it lifted him the look of houses and fathers of the ledge of the corn. The sea, while you ask\n",
            "moodily up. What does not have a hard talk of mighty men.\n",
            "\n",
            "He took his finger, attempted the child again and would lean himself strong and\n",
            "struggling, but when a gash was his order was uncomfortanced no more.\n",
            "\n",
            "Now call ear with your things. Least arone, man went out to the\n",
            "deer, or some of a recluse at MrandMrimoman of the Fianna of Ireland,\n",
            "retreating the tracks a skept and crawled along the river. At such any power they would\n",
            "least out at the air alently that she said, \"You be looking seeks\n",
            "lost sidels are sickned and hurled,\" from that rock, and confisting the deserted\n",
            "and shining fruit, and, crying it and sailed for dread happily, the family of the\n",
            "big rake caution in facts of small gay campers, and in blue, others\n",
            "were of rowing over the Indian language, they came back with a\n",
            "fish. He saw his fine hundred feet on the table, near\n",
            "the less of seven \n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the character generator\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "\n",
        "states = None\n",
        "\n",
        "# Set what we want the beginning prompt to be, and let the model finish the rest!\n",
        "next_char = tf.constant(['The world seemed like such a peaceful place until the magic tree was discovered in London.'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(5000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "\n",
        "# Print the results formatted.\n",
        "print(result[0].numpy().decode('utf-8'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}